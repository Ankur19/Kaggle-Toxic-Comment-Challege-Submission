{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import dependencies\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import h5py\n",
    "import unidecode\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Dense, Conv1D, Dropout, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import backend as K\n",
    "from scipy.sparse import vstack\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "levels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "#summing the toxicity levels so that we can easily divide the train data to K folds.\n",
    "train['sum_level'] = train[levels[0]] + train[levels[1]] + train[levels[2]] + train[levels[3]] + train[levels[4]] + train[levels[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [00:07<00:00, 20927.67it/s]\n",
      "100%|██████████| 153164/153164 [00:07<00:00, 20472.66it/s]\n",
      "100%|██████████| 159571/159571 [00:09<00:00, 16448.93it/s]\n",
      "100%|██████████| 153164/153164 [00:12<00:00, 12572.25it/s]\n",
      "100%|██████████| 159571/159571 [00:40<00:00, 3987.37it/s]\n",
      "100%|██████████| 153164/153164 [00:45<00:00, 3373.46it/s]\n"
     ]
    }
   ],
   "source": [
    "#we see there are many \\n characters in text. lets just remove those first\n",
    "good_text = []\n",
    "for i in tqdm(train['comment_text']):\n",
    "    i = re.sub(r'[\\n]+', ' ', i)\n",
    "    i = re.sub(r'\\s+', ' ', i)\n",
    "    good_text.append(i)\n",
    "train['comment_text'] = good_text\n",
    "\n",
    "\n",
    "#we see there are many \\n characters in text. lets just remove those first\n",
    "good_text_test = []\n",
    "for i in tqdm(test['comment_text']):\n",
    "    i = re.sub(r'[\\n]+', ' ', i)\n",
    "    i = re.sub(r'\\s+', ' ', i)\n",
    "    good_text_test.append(i)\n",
    "test['comment_text'] = good_text_test\n",
    "\n",
    "\n",
    "#let us strip the unicode accents\n",
    "\n",
    "good_text = []\n",
    "for i in tqdm(train['comment_text']):\n",
    "    i = unicode(i, 'utf-8')\n",
    "    i = unidecode.unidecode(i)\n",
    "    good_text.append(i)\n",
    "train['comment_text'] = good_text\n",
    "\n",
    "\n",
    "good_text_test = []\n",
    "for i in tqdm(test['comment_text']):\n",
    "    i = unicode(i, 'utf-8')\n",
    "    i = unidecode.unidecode(i)\n",
    "    good_text_test.append(i)\n",
    "test['comment_text'] = good_text_test\n",
    "\n",
    "\n",
    "good_text = []\n",
    "for i in tqdm(train['comment_text']):\n",
    "    i = i.lower()\n",
    "    i = re.sub(r'\\\\\\'s', ' is', i)\n",
    "    i = re.sub(r'\\'s', ' is', i)\n",
    "    \n",
    "    i = re.sub(r'can\\\\\\'t', 'can not', i)\n",
    "    i = re.sub(r'can\\'t', 'can not', i)\n",
    "    \n",
    "    i = re.sub(r'n\\\\\\'t', ' not', i)\n",
    "    i = re.sub(r'n\\'t', ' not', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'nt', ' not', i)\n",
    "    i = re.sub(r'\\'nt', ' not', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'re', ' are', i)\n",
    "    i = re.sub(r'\\'re', ' are', i)\n",
    "    \n",
    "    i = re.sub(r'\\s[w]\\'d', ' would', i)\n",
    "    i = re.sub(r'\\\\\\'d', ' would', i)\n",
    "    i = re.sub(r'\\'d', ' would', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'ll', ' will', i)\n",
    "    i = re.sub(r'\\'ll', ' will', i)\n",
    "    \n",
    "    i = re.sub(r'i\\\\\\'m', ' i am ', i)\n",
    "    i = re.sub(r'i\\'m', ' i am ', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'pedia', ' wikipedia ', i)\n",
    "    i = re.sub(r'\\'pedia', ' wikipedia ', i)\n",
    "    \n",
    "    i = re.sub(r'https://www\\.', ' www ', i)\n",
    "    i = re.sub(r'www\\.', ' www ', i)\n",
    "    \n",
    "    i = re.sub(r'\\.com', ' com ', i)\n",
    "    \n",
    "    i = re.sub(r'[-]+', ' ', i)\n",
    "    \n",
    "    i = re.sub(r'[\\[ \\] \\. \" # \\$ % \\^ \\* \\( \\) \\? \\\\ / @ < > _ : = \\+ \\{ } \\| ~ ! , \\']+', ' ', i)\n",
    "    \n",
    "    i = re.sub(r'\\s+', ' ', i)\n",
    "    \n",
    "    i = i.strip()\n",
    "    \n",
    "    good_text.append(i)\n",
    "train['comment_text'] = good_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "good_text_test= []\n",
    "for i in tqdm(test['comment_text']):\n",
    "    i = i.lower()\n",
    "    i = re.sub(r'\\\\\\'s', ' is', i)\n",
    "    i = re.sub(r'\\'s', ' is', i)\n",
    "    \n",
    "    i = re.sub(r'can\\\\\\'t', 'can not', i)\n",
    "    i = re.sub(r'can\\'t', 'can not', i)\n",
    "    \n",
    "    i = re.sub(r'n\\\\\\'t', ' not', i)\n",
    "    i = re.sub(r'n\\'t', ' not', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'nt', ' not', i)\n",
    "    i = re.sub(r'\\'nt', ' not', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'re', ' are', i)\n",
    "    i = re.sub(r'\\'re', ' are', i)\n",
    "    \n",
    "    i = re.sub(r'\\s[w]\\'d', ' would', i)\n",
    "    i = re.sub(r'\\\\\\'d', ' would', i)\n",
    "    i = re.sub(r'\\'d', ' would', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'ll', ' will', i)\n",
    "    i = re.sub(r'\\'ll', ' will', i)\n",
    "    \n",
    "    i = re.sub(r'i\\\\\\'m', ' i am ', i)\n",
    "    i = re.sub(r'i\\'m', ' i am ', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'pedia', ' wikipedia ', i)\n",
    "    i = re.sub(r'\\'pedia', ' wikipedia ', i)\n",
    "    \n",
    "    i = re.sub(r'https://www\\.', ' www ', i)\n",
    "    i = re.sub(r'www\\.', ' www ', i)\n",
    "    \n",
    "    i = re.sub(r'\\.com', ' com ', i)\n",
    "    \n",
    "    i = re.sub(r'[-]+', ' ', i)\n",
    "    \n",
    "    i = re.sub(r'[\\[ \\] \\. \" # \\$ % \\^ \\* \\( \\) \\? \\\\ / @ < > _ : = \\+ \\{ } \\| ~ ! , \\']+', ' ', i)\n",
    "    \n",
    "    i = re.sub(r'\\s+', ' ', i)\n",
    "    \n",
    "    i = i.strip()\n",
    "    \n",
    "    good_text_test.append(i)\n",
    "test['comment_text'] = good_text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['line_length'] = train['comment_text'].apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.2807465015573"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['line_length'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 340522 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "all_text = pd.concat([train['comment_text'], test['comment_text']])\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "train_seq = tokenizer.texts_to_sequences(train['comment_text'])\n",
    "test_seq = tokenizer.texts_to_sequences(test['comment_text'])\n",
    "\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "\n",
    "\n",
    "train_seq = pad_sequences(train_seq, maxlen=150)\n",
    "test_seq = pad_sequences(test_seq, maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [05:57, 6144.98it/s] \n",
      "  6%|▋         | 21708/340522 [00:00<00:01, 217067.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340522/340522 [00:01<00:00, 270227.49it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.840B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "ii=0\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        ii+=1\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(\"words found in glove: \" + str(ii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('embedding_matrix.txt',embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.loadtxt('embedding_matrix.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_clean_index = train[train['sum_level']==0].index.values\n",
    "train_toxic_index = train[train['sum_level']>0].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.concatenate((train_seq[train_clean_index[28670*3:28670*4],:], train_seq[train_toxic_index,:]), axis=0)\n",
    "y = np.concatenate((np.array(train.iloc[train_clean_index[28670*3:28670*4],2:-2]), np.array(train.iloc[train_toxic_index,2:-2])), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class roc_callback(Callback):\n",
    "    def __init__(self,training_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.x)\n",
    "        roc = roc_auc_score(self.y, y_pred, average='weighted')\n",
    "        print('\\rroc-auc: %s' % (str(round(roc,4)))+' '+'\\n')\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 300)          102156900 \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 149, 512)          307712    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 149, 512)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 74, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 73, 512)           524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 73, 512)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 37376)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 37376)             149504    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               9568512   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 112,906,346\n",
      "Trainable params: 10,674,694\n",
      "Non-trainable params: 102,231,652\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D,MaxPooling1D,Flatten,Embedding, BatchNormalization,AveragePooling1D,GlobalMaxPooling1D\n",
    "\n",
    "\n",
    "inputs = Input(shape=(150,))\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],input_length=150,trainable=False)(inputs)\n",
    "x = Conv1D(512, 2, activation='tanh', padding='valid')(embedding_layer)\n",
    "x = Dropout(0.5)(x)\n",
    "x = MaxPooling1D(2)(x)\n",
    "x = Conv1D(512, 2, activation='tanh')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "#x = GlobalMaxPooling1D(2)(x)\n",
    "x = Flatten()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256, activation='tanh')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='tanh')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='tanh')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='tanh')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output= Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "mck = ModelCheckpoint('sub_conv1d.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "estop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25141 samples, validate on 10775 samples\n",
      "Epoch 1/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.2391 - categorical_accuracy: 0.5194Epoch 00000: val_loss improved from inf to 1.25014, saving model to sub_conv1d.h5\n",
      "roc-auc: 0.6649 \n",
      "\n",
      "25141/25141 [==============================] - 20s - loss: 1.2390 - categorical_accuracy: 0.5204 - val_loss: 1.2501 - val_categorical_accuracy: 0.8431\n",
      "Epoch 2/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.1551 - categorical_accuracy: 0.7277Epoch 00001: val_loss improved from 1.25014 to 1.18370, saving model to sub_conv1d.h5\n",
      "roc-auc: 0.7174 \n",
      "\n",
      "25141/25141 [==============================] - 19s - loss: 1.1545 - categorical_accuracy: 0.7282 - val_loss: 1.1837 - val_categorical_accuracy: 0.8466\n",
      "Epoch 3/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.1264 - categorical_accuracy: 0.8356Epoch 00002: val_loss improved from 1.18370 to 1.13159, saving model to sub_conv1d.h5\n",
      "roc-auc: 0.7506 \n",
      "\n",
      "25141/25141 [==============================] - 19s - loss: 1.1260 - categorical_accuracy: 0.8358 - val_loss: 1.1316 - val_categorical_accuracy: 0.9178\n",
      "Epoch 4/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.1180 - categorical_accuracy: 0.8853Epoch 00003: val_loss improved from 1.13159 to 1.12517, saving model to sub_conv1d.h5\n",
      "roc-auc: 0.766 \n",
      "\n",
      "25141/25141 [==============================] - 19s - loss: 1.1159 - categorical_accuracy: 0.8855 - val_loss: 1.1252 - val_categorical_accuracy: 0.9079\n",
      "Epoch 5/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.1071 - categorical_accuracy: 0.9102Epoch 00004: val_loss improved from 1.12517 to 1.09900, saving model to sub_conv1d.h5\n",
      "roc-auc: 0.7922 \n",
      "\n",
      "25141/25141 [==============================] - 19s - loss: 1.1069 - categorical_accuracy: 0.9102 - val_loss: 1.0990 - val_categorical_accuracy: 0.9279\n",
      "Epoch 6/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0970 - categorical_accuracy: 0.9387Epoch 00005: val_loss improved from 1.09900 to 1.09182, saving model to sub_conv1d.h5\n",
      "roc-auc: 0.8197 \n",
      "\n",
      "25141/25141 [==============================] - 19s - loss: 1.0969 - categorical_accuracy: 0.9384 - val_loss: 1.0918 - val_categorical_accuracy: 0.9122\n",
      "Epoch 7/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0955 - categorical_accuracy: 0.9382Epoch 00006: val_loss improved from 1.09182 to 1.07999, saving model to sub_conv1d.h5\n",
      "roc-auc: 0.854 \n",
      "\n",
      "25141/25141 [==============================] - 19s - loss: 1.0955 - categorical_accuracy: 0.9384 - val_loss: 1.0800 - val_categorical_accuracy: 0.9236\n",
      "Epoch 8/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0850 - categorical_accuracy: 0.9504Epoch 00007: val_loss did not improve\n",
      "roc-auc: 0.8598 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 1.0831 - categorical_accuracy: 0.9505 - val_loss: 1.0803 - val_categorical_accuracy: 0.9659\n",
      "Epoch 9/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0713 - categorical_accuracy: 0.9628Epoch 00008: val_loss improved from 1.07999 to 1.06648, saving model to sub_conv1d.h5\n",
      "roc-auc: 0.8441 \n",
      "\n",
      "25141/25141 [==============================] - 19s - loss: 1.0713 - categorical_accuracy: 0.9627 - val_loss: 1.0665 - val_categorical_accuracy: 0.9746\n",
      "Epoch 10/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0666 - categorical_accuracy: 0.9708Epoch 00009: val_loss did not improve\n",
      "roc-auc: 0.8379 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 1.0665 - categorical_accuracy: 0.9708 - val_loss: 1.0893 - val_categorical_accuracy: 0.8901\n",
      "Epoch 11/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0643 - categorical_accuracy: 0.9689Epoch 00010: val_loss improved from 1.06648 to 1.06049, saving model to sub_conv1d.h5\n",
      "roc-auc: 0.863 \n",
      "\n",
      "25141/25141 [==============================] - 19s - loss: 1.0632 - categorical_accuracy: 0.9691 - val_loss: 1.0605 - val_categorical_accuracy: 0.9572\n",
      "Epoch 12/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0521 - categorical_accuracy: 0.9760Epoch 00011: val_loss did not improve\n",
      "roc-auc: 0.8625 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 1.0522 - categorical_accuracy: 0.9758 - val_loss: 1.0619 - val_categorical_accuracy: 0.9560\n",
      "Epoch 13/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0517 - categorical_accuracy: 0.9744Epoch 00012: val_loss improved from 1.06049 to 1.05271, saving model to sub_conv1d.h5\n",
      "roc-auc: 0.8467 \n",
      "\n",
      "25141/25141 [==============================] - 19s - loss: 1.0517 - categorical_accuracy: 0.9745 - val_loss: 1.0527 - val_categorical_accuracy: 0.9787\n",
      "Epoch 14/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0532 - categorical_accuracy: 0.9714Epoch 00013: val_loss did not improve\n",
      "roc-auc: 0.8451 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 1.0520 - categorical_accuracy: 0.9714 - val_loss: 1.0917 - val_categorical_accuracy: 0.9194\n",
      "Epoch 15/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0428 - categorical_accuracy: 0.9770Epoch 00014: val_loss did not improve\n",
      "roc-auc: 0.8603 \n",
      "\n",
      "25141/25141 [==============================] - 16s - loss: 1.0433 - categorical_accuracy: 0.9769 - val_loss: 1.0766 - val_categorical_accuracy: 0.9757\n",
      "Epoch 16/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0395 - categorical_accuracy: 0.9754Epoch 00015: val_loss did not improve\n",
      "roc-auc: 0.8331 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 1.0391 - categorical_accuracy: 0.9754 - val_loss: 1.1090 - val_categorical_accuracy: 0.9645\n",
      "Epoch 17/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0352 - categorical_accuracy: 0.9758Epoch 00016: val_loss improved from 1.05271 to 1.05050, saving model to sub_conv1d.h5\n",
      "roc-auc: 0.8784 \n",
      "\n",
      "25141/25141 [==============================] - 19s - loss: 1.0357 - categorical_accuracy: 0.9759 - val_loss: 1.0505 - val_categorical_accuracy: 0.9778\n",
      "Epoch 18/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0308 - categorical_accuracy: 0.9738Epoch 00017: val_loss did not improve\n",
      "roc-auc: 0.8324 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 1.0319 - categorical_accuracy: 0.9739 - val_loss: 1.0852 - val_categorical_accuracy: 0.9763\n",
      "Epoch 19/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0316 - categorical_accuracy: 0.9738Epoch 00018: val_loss did not improve\n",
      "roc-auc: 0.8169 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 1.0315 - categorical_accuracy: 0.9739 - val_loss: 1.1255 - val_categorical_accuracy: 0.8898\n",
      "Epoch 20/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0248 - categorical_accuracy: 0.9712Epoch 00019: val_loss did not improve\n",
      "roc-auc: 0.8392 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 1.0239 - categorical_accuracy: 0.9711 - val_loss: 1.0782 - val_categorical_accuracy: 0.9335\n",
      "Epoch 21/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0168 - categorical_accuracy: 0.9748Epoch 00020: val_loss did not improve\n",
      "roc-auc: 0.8255 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 1.0178 - categorical_accuracy: 0.9748 - val_loss: 1.0961 - val_categorical_accuracy: 0.9203\n",
      "Epoch 22/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0171 - categorical_accuracy: 0.9761Epoch 00021: val_loss did not improve\n",
      "roc-auc: 0.8097 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 1.0158 - categorical_accuracy: 0.9761 - val_loss: 1.1268 - val_categorical_accuracy: 0.8580\n",
      "Epoch 23/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0107 - categorical_accuracy: 0.9740Epoch 00022: val_loss did not improve\n",
      "roc-auc: 0.8104 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 1.0110 - categorical_accuracy: 0.9739 - val_loss: 1.1077 - val_categorical_accuracy: 0.9018\n",
      "Epoch 24/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0085 - categorical_accuracy: 0.9747Epoch 00023: val_loss did not improve\n",
      "roc-auc: 0.8328 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 1.0080 - categorical_accuracy: 0.9747 - val_loss: 1.0973 - val_categorical_accuracy: 0.9058\n",
      "Epoch 25/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9994 - categorical_accuracy: 0.9749Epoch 00024: val_loss did not improve\n",
      "roc-auc: 0.8276 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 0.9997 - categorical_accuracy: 0.9749 - val_loss: 1.0870 - val_categorical_accuracy: 0.9234\n",
      "Epoch 26/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9959 - categorical_accuracy: 0.9739Epoch 00025: val_loss did not improve\n",
      "roc-auc: 0.8339 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 0.9958 - categorical_accuracy: 0.9740 - val_loss: 1.0520 - val_categorical_accuracy: 0.9651\n",
      "Epoch 27/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9997 - categorical_accuracy: 0.9741Epoch 00026: val_loss did not improve\n",
      "roc-auc: 0.8478 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 1.0003 - categorical_accuracy: 0.9741 - val_loss: 1.0583 - val_categorical_accuracy: 0.9487\n",
      "Epoch 28/100\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9956 - categorical_accuracy: 0.9749Epoch 00027: val_loss did not improve\n",
      "roc-auc: 0.839 \n",
      "\n",
      "25141/25141 [==============================] - 17s - loss: 0.9962 - categorical_accuracy: 0.9750 - val_loss: 1.0956 - val_categorical_accuracy: 0.9348\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f74a7147f50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=100, batch_size=1000, verbose=1, shuffle=True, validation_split=0.3, callbacks=[mck,estop,roc_callback(training_data=(x_train,y_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.877316403448\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('sub_conv1d.h5')\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = roc_auc_score(y_test,y_pred,average='weighted')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(test['id'])\n",
    "\n",
    "preds = model.predict(test_seq)\n",
    "\n",
    "for i in range(len(levels)):\n",
    "    sub[levels[i]]=preds[:,i]\n",
    "sub.to_csv('final_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2, random_state=65)#7365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 300)          102156900 \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               1140736   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 103,693,674\n",
      "Trainable params: 1,536,774\n",
      "Non-trainable params: 102,156,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D,MaxPooling1D,Flatten,Embedding, BatchNormalization,AveragePooling1D,GlobalMaxPooling1D,Bidirectional, LSTM\n",
    "\n",
    "\n",
    "inputs = Input(shape=(150,))\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],input_length=150,trainable=False)(inputs)\n",
    "x = Bidirectional(LSTM(256, activation='tanh', dropout=0.3,recurrent_dropout=0.3),merge_mode='concat')(embedding_layer)\n",
    "#x = BatchNormalization()(x)\n",
    "x = Dense(256, activation='tanh')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='tanh')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='tanh')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='tanh')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='tanh')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output= Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "mck = ModelCheckpoint('sub_conv1d_new.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "estop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25141 samples, validate on 10775 samples\n",
      "Epoch 1/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.1333 - categorical_accuracy: 0.9466Epoch 00000: val_loss improved from inf to 1.02647, saving model to sub_conv1d_new.h5\n",
      "roc-auc: 0.8295 \n",
      "\n",
      "25141/25141 [==============================] - 369s - loss: 1.1346 - categorical_accuracy: 0.9467 - val_loss: 1.0265 - val_categorical_accuracy: 0.9790\n",
      "Epoch 2/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0891 - categorical_accuracy: 0.9682Epoch 00001: val_loss improved from 1.02647 to 1.01487, saving model to sub_conv1d_new.h5\n",
      "roc-auc: 0.8363 \n",
      "\n",
      "25141/25141 [==============================] - 306s - loss: 1.0897 - categorical_accuracy: 0.9683 - val_loss: 1.0149 - val_categorical_accuracy: 0.9788\n",
      "Epoch 3/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0592 - categorical_accuracy: 0.9712Epoch 00002: val_loss improved from 1.01487 to 0.99902, saving model to sub_conv1d_new.h5\n",
      "roc-auc: 0.7121 \n",
      "\n",
      "25141/25141 [==============================] - 291s - loss: 1.0621 - categorical_accuracy: 0.9711 - val_loss: 0.9990 - val_categorical_accuracy: 0.9787\n",
      "Epoch 4/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0412 - categorical_accuracy: 0.9713Epoch 00003: val_loss improved from 0.99902 to 0.97901, saving model to sub_conv1d_new.h5\n",
      "roc-auc: 0.7504 \n",
      "\n",
      "25141/25141 [==============================] - 305s - loss: 1.0408 - categorical_accuracy: 0.9713 - val_loss: 0.9790 - val_categorical_accuracy: 0.9786\n",
      "Epoch 5/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0206 - categorical_accuracy: 0.9732Epoch 00004: val_loss improved from 0.97901 to 0.96679, saving model to sub_conv1d_new.h5\n",
      "roc-auc: 0.7956 \n",
      "\n",
      "25141/25141 [==============================] - 286s - loss: 1.0215 - categorical_accuracy: 0.9733 - val_loss: 0.9668 - val_categorical_accuracy: 0.9791\n",
      "Epoch 6/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0093 - categorical_accuracy: 0.9740Epoch 00005: val_loss improved from 0.96679 to 0.96665, saving model to sub_conv1d_new.h5\n",
      "roc-auc: 0.891 \n",
      "\n",
      "25141/25141 [==============================] - 282s - loss: 1.0093 - categorical_accuracy: 0.9740 - val_loss: 0.9667 - val_categorical_accuracy: 0.9791\n",
      "Epoch 7/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0020 - categorical_accuracy: 0.9758Epoch 00006: val_loss improved from 0.96665 to 0.96048, saving model to sub_conv1d_new.h5\n",
      "roc-auc: 0.8348 \n",
      "\n",
      "25141/25141 [==============================] - 285s - loss: 1.0014 - categorical_accuracy: 0.9756 - val_loss: 0.9605 - val_categorical_accuracy: 0.9791\n",
      "Epoch 8/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9950 - categorical_accuracy: 0.9747Epoch 00007: val_loss did not improve\n",
      "roc-auc: 0.6716 \n",
      "\n",
      "25141/25141 [==============================] - 304s - loss: 0.9942 - categorical_accuracy: 0.9747 - val_loss: 0.9646 - val_categorical_accuracy: 0.9791\n",
      "Epoch 9/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9878 - categorical_accuracy: 0.9755Epoch 00008: val_loss improved from 0.96048 to 0.95927, saving model to sub_conv1d_new.h5\n",
      "roc-auc: 0.7056 \n",
      "\n",
      "25141/25141 [==============================] - 314s - loss: 0.9867 - categorical_accuracy: 0.9755 - val_loss: 0.9593 - val_categorical_accuracy: 0.9791\n",
      "Epoch 10/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9826 - categorical_accuracy: 0.9748Epoch 00009: val_loss did not improve\n",
      "roc-auc: 0.8364 \n",
      "\n",
      "25141/25141 [==============================] - 295s - loss: 0.9812 - categorical_accuracy: 0.9748 - val_loss: 0.9598 - val_categorical_accuracy: 0.9791\n",
      "Epoch 11/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9757 - categorical_accuracy: 0.9748Epoch 00010: val_loss improved from 0.95927 to 0.95847, saving model to sub_conv1d_new.h5\n",
      "roc-auc: 0.7502 \n",
      "\n",
      "25141/25141 [==============================] - 295s - loss: 0.9763 - categorical_accuracy: 0.9749 - val_loss: 0.9585 - val_categorical_accuracy: 0.9790\n",
      "Epoch 12/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9669 - categorical_accuracy: 0.9750Epoch 00011: val_loss did not improve\n",
      "roc-auc: 0.8917 \n",
      "\n",
      "25141/25141 [==============================] - 302s - loss: 0.9672 - categorical_accuracy: 0.9751 - val_loss: 0.9671 - val_categorical_accuracy: 0.9790\n",
      "Epoch 13/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9651 - categorical_accuracy: 0.9749Epoch 00012: val_loss did not improve\n",
      "roc-auc: 0.812 \n",
      "\n",
      "25141/25141 [==============================] - 284s - loss: 0.9665 - categorical_accuracy: 0.9749 - val_loss: 0.9585 - val_categorical_accuracy: 0.9791\n",
      "Epoch 14/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9594 - categorical_accuracy: 0.9741Epoch 00013: val_loss did not improve\n",
      "roc-auc: 0.6749 \n",
      "\n",
      "25141/25141 [==============================] - 281s - loss: 0.9589 - categorical_accuracy: 0.9740 - val_loss: 0.9612 - val_categorical_accuracy: 0.9790\n",
      "Epoch 15/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9583 - categorical_accuracy: 0.9741Epoch 00014: val_loss did not improve\n",
      "roc-auc: 0.91 \n",
      "\n",
      "25141/25141 [==============================] - 280s - loss: 0.9581 - categorical_accuracy: 0.9742 - val_loss: 0.9723 - val_categorical_accuracy: 0.9790\n",
      "Epoch 16/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9489 - categorical_accuracy: 0.9710Epoch 00015: val_loss did not improve\n",
      "roc-auc: 0.8357 \n",
      "\n",
      "25141/25141 [==============================] - 302s - loss: 0.9493 - categorical_accuracy: 0.9712 - val_loss: 0.9718 - val_categorical_accuracy: 0.9791\n",
      "Epoch 17/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9463 - categorical_accuracy: 0.9745Epoch 00016: val_loss did not improve\n",
      "roc-auc: 0.8747 \n",
      "\n",
      "25141/25141 [==============================] - 354s - loss: 0.9461 - categorical_accuracy: 0.9746 - val_loss: 0.9661 - val_categorical_accuracy: 0.9790\n",
      "Epoch 18/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9612 - categorical_accuracy: 0.9762Epoch 00017: val_loss did not improve\n",
      "roc-auc: 0.7946 \n",
      "\n",
      "25141/25141 [==============================] - 326s - loss: 0.9605 - categorical_accuracy: 0.9761 - val_loss: 0.9660 - val_categorical_accuracy: 0.9791\n",
      "Epoch 19/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9453 - categorical_accuracy: 0.9737Epoch 00018: val_loss did not improve\n",
      "roc-auc: 0.9026 \n",
      "\n",
      "25141/25141 [==============================] - 295s - loss: 0.9451 - categorical_accuracy: 0.9736 - val_loss: 0.9689 - val_categorical_accuracy: 0.9790\n",
      "Epoch 20/20\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 0.9365 - categorical_accuracy: 0.9750Epoch 00019: val_loss did not improve\n",
      "roc-auc: 0.8708 \n",
      "\n",
      "25141/25141 [==============================] - 335s - loss: 0.9367 - categorical_accuracy: 0.9751 - val_loss: 0.9747 - val_categorical_accuracy: 0.9790\n",
      "Epoch 00019: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4dd6973e90>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=20, batch_size=200, verbose=1, shuffle=True, validation_split=0.3, callbacks=[mck,estop,roc_callback(training_data=(x_train,y_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('sub_conv1d_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746647545169\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "score = roc_auc_score(y_test,y_pred,average='weighted')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(test['id'])\n",
    "\n",
    "preds = model.predict(test_seq)\n",
    "\n",
    "for i in range(len(levels)):\n",
    "    sub[levels[i]]=preds[:,i]\n",
    "sub.to_csv('final_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
