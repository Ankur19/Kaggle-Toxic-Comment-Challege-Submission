{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import h5py\n",
    "import unidecode\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Dense, Conv1D, Dropout, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import backend as K\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "levels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "#summing the toxicity levels so that we can easily divide the train data to K folds.\n",
    "train['sum_level'] = train[levels[0]] + train[levels[1]] + train[levels[2]] + train[levels[3]] + train[levels[4]] + train[levels[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we see there are many \\n characters in text. lets just remove those first\n",
    "good_text = []\n",
    "for i in tqdm(train['comment_text']):\n",
    "    i = re.sub(r'[\\n]+', ' ', i)\n",
    "    i = re.sub(r'\\s+', ' ', i)\n",
    "    good_text.append(i)\n",
    "train['comment_text'] = good_text\n",
    "\n",
    "\n",
    "#we see there are many \\n characters in text. lets just remove those first\n",
    "good_text_test = []\n",
    "for i in tqdm(test['comment_text']):\n",
    "    i = re.sub(r'[\\n]+', ' ', i)\n",
    "    i = re.sub(r'\\s+', ' ', i)\n",
    "    good_text_test.append(i)\n",
    "test['comment_text'] = good_text_test\n",
    "\n",
    "\n",
    "#let us strip the unicode accents\n",
    "\n",
    "good_text = []\n",
    "for i in tqdm(train['comment_text']):\n",
    "    i = unicode(i, 'utf-8')\n",
    "    i = unidecode.unidecode(i)\n",
    "    good_text.append(i)\n",
    "train['comment_text'] = good_text\n",
    "\n",
    "\n",
    "good_text_test = []\n",
    "for i in tqdm(test['comment_text']):\n",
    "    i = unicode(i, 'utf-8')\n",
    "    i = unidecode.unidecode(i)\n",
    "    good_text_test.append(i)\n",
    "test['comment_text'] = good_text_test\n",
    "\n",
    "\n",
    "good_text = []\n",
    "for i in tqdm(train['comment_text']):\n",
    "    i = i.lower()\n",
    "    i = re.sub(r'\\\\\\'s', ' is', i)\n",
    "    i = re.sub(r'\\'s', ' is', i)\n",
    "    \n",
    "    i = re.sub(r'can\\\\\\'t', 'can not', i)\n",
    "    i = re.sub(r'can\\'t', 'can not', i)\n",
    "    \n",
    "    i = re.sub(r'n\\\\\\'t', ' not', i)\n",
    "    i = re.sub(r'n\\'t', ' not', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'nt', ' not', i)\n",
    "    i = re.sub(r'\\'nt', ' not', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'re', ' are', i)\n",
    "    i = re.sub(r'\\'re', ' are', i)\n",
    "    \n",
    "    i = re.sub(r'\\s[w]\\'d', ' would', i)\n",
    "    i = re.sub(r'\\\\\\'d', ' would', i)\n",
    "    i = re.sub(r'\\'d', ' would', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'ll', ' will', i)\n",
    "    i = re.sub(r'\\'ll', ' will', i)\n",
    "    \n",
    "    i = re.sub(r'i\\\\\\'m', ' i am ', i)\n",
    "    i = re.sub(r'i\\'m', ' i am ', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'pedia', ' wikipedia ', i)\n",
    "    i = re.sub(r'\\'pedia', ' wikipedia ', i)\n",
    "    \n",
    "    i = re.sub(r'https://www\\.', ' www ', i)\n",
    "    i = re.sub(r'www\\.', ' www ', i)\n",
    "    \n",
    "    i = re.sub(r'\\.com', ' com ', i)\n",
    "    \n",
    "    i = re.sub(r'[-]+', ' ', i)\n",
    "    \n",
    "    i = re.sub(r'[\\[ \\] \\. \" # \\$ % \\^ \\* \\( \\) \\? \\\\ / @ < > _ : = \\+ \\{ } \\| ~ ! , \\']+', ' ', i)\n",
    "    \n",
    "    i = re.sub(r'\\s+', ' ', i)\n",
    "    \n",
    "    i = i.strip()\n",
    "    \n",
    "    good_text.append(i)\n",
    "train['comment_text'] = good_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "good_text_test= []\n",
    "for i in tqdm(test['comment_text']):\n",
    "    i = i.lower()\n",
    "    i = re.sub(r'\\\\\\'s', ' is', i)\n",
    "    i = re.sub(r'\\'s', ' is', i)\n",
    "    \n",
    "    i = re.sub(r'can\\\\\\'t', 'can not', i)\n",
    "    i = re.sub(r'can\\'t', 'can not', i)\n",
    "    \n",
    "    i = re.sub(r'n\\\\\\'t', ' not', i)\n",
    "    i = re.sub(r'n\\'t', ' not', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'nt', ' not', i)\n",
    "    i = re.sub(r'\\'nt', ' not', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'re', ' are', i)\n",
    "    i = re.sub(r'\\'re', ' are', i)\n",
    "    \n",
    "    i = re.sub(r'\\s[w]\\'d', ' would', i)\n",
    "    i = re.sub(r'\\\\\\'d', ' would', i)\n",
    "    i = re.sub(r'\\'d', ' would', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'ll', ' will', i)\n",
    "    i = re.sub(r'\\'ll', ' will', i)\n",
    "    \n",
    "    i = re.sub(r'i\\\\\\'m', ' i am ', i)\n",
    "    i = re.sub(r'i\\'m', ' i am ', i)\n",
    "    \n",
    "    i = re.sub(r'\\\\\\'pedia', ' wikipedia ', i)\n",
    "    i = re.sub(r'\\'pedia', ' wikipedia ', i)\n",
    "    \n",
    "    i = re.sub(r'https://www\\.', ' www ', i)\n",
    "    i = re.sub(r'www\\.', ' www ', i)\n",
    "    \n",
    "    i = re.sub(r'\\.com', ' com ', i)\n",
    "    \n",
    "    i = re.sub(r'[-]+', ' ', i)\n",
    "    \n",
    "    i = re.sub(r'[\\[ \\] \\. \" # \\$ % \\^ \\* \\( \\) \\? \\\\ / @ < > _ : = \\+ \\{ } \\| ~ ! , \\']+', ' ', i)\n",
    "    \n",
    "    i = re.sub(r'\\s+', ' ', i)\n",
    "    \n",
    "    i = i.strip()\n",
    "    \n",
    "    good_text_test.append(i)\n",
    "test['comment_text'] = good_text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_train_1 = np.loadtxt('train_tfidf_data_1.txt')\n",
    "tfidf_test_1 = np.loadtxt('test_tfidf_data_1.txt')\n",
    "\n",
    "tfidf_train_2 = np.loadtxt('train_tfidf_data_2.txt')\n",
    "tfidf_test_2 = np.loadtxt('test_tfidf_data_2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have 10 times more non toxic data than toxic data. Let us divide the non toxic comments into 5 pars and train 5 models with each part each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143346, 9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['sum_level']==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16225, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['sum_level']>0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28669"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "143346/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_clean_index = train[train['sum_level']==0].index.values\n",
    "train_toxic_index = train[train['sum_level']>0].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((tfidf_train_1[train_clean_index[:28670],:], tfidf_train_1[train_toxic_index,:]), axis=0)\n",
    "y = np.concatenate((np.array(train.iloc[train_clean_index[:28670],2:-1]), np.array(train.iloc[train_toxic_index,2:-1])), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(x.shape[0]==y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2, random_state=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class roc_callback(Callback):\n",
    "    def __init__(self,training_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.x)\n",
    "        roc = roc_auc_score(self.y, y_pred, average='weighted')\n",
    "        print('\\rroc-auc: %s' % (str(round(roc,4)))+' '+'\\n')\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               25728     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 76,038\n",
      "Trainable params: 76,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#now we are ready for Deep learning. Wanted to start with boosting, but lets strt with DL. \n",
    "#we will start with a simple keras model\n",
    "\n",
    "#considering text data let us start with a simple fully connected model\n",
    "\n",
    "\n",
    "#starting keras model with tensorflow backend\n",
    "\n",
    "inputs = Input(shape=(200,))\n",
    "\n",
    "a = Dense(128,activation='tanh', input_shape=(None,200))(inputs)\n",
    "a = Dropout(0.6)(a)\n",
    "a = Dense(128,activation='tanh')(a)\n",
    "a = Dropout(0.6)(a)\n",
    "a = Dense(128,activation='tanh')(a)\n",
    "a = Dropout(0.6)(a)\n",
    "a = Dense(128,activation='tanh')(a)\n",
    "a = Dropout(0.6)(a)\n",
    "output= Dense(6, activation='sigmoid')(a)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "mck = ModelCheckpoint('sub_x1.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "estop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25141 samples, validate on 10775 samples\n",
      "Epoch 1/200\n",
      "24720/25141 [============================>.] - ETA: 0s - loss: 1.0824 - categorical_accuracy: 0.9435Epoch 00000: val_loss improved from inf to 1.05165, saving model to sub_x1.h5\n",
      "roc-auc: 0.6644 \n",
      "\n",
      "25141/25141 [==============================] - 5s - loss: 1.0831 - categorical_accuracy: 0.9442 - val_loss: 1.0517 - val_categorical_accuracy: 0.9786\n",
      "Epoch 2/200\n",
      "24720/25141 [============================>.] - ETA: 0s - loss: 1.0307 - categorical_accuracy: 0.9690Epoch 00001: val_loss improved from 1.05165 to 1.02609, saving model to sub_x1.h5\n",
      "roc-auc: 0.7303 \n",
      "\n",
      "25141/25141 [==============================] - 4s - loss: 1.0305 - categorical_accuracy: 0.9691 - val_loss: 1.0261 - val_categorical_accuracy: 0.9770\n",
      "Epoch 3/200\n",
      "24640/25141 [============================>.] - ETA: 0s - loss: 1.0165 - categorical_accuracy: 0.9683Epoch 00002: val_loss improved from 1.02609 to 1.01996, saving model to sub_x1.h5\n",
      "roc-auc: 0.7013 \n",
      "\n",
      "25141/25141 [==============================] - 4s - loss: 1.0163 - categorical_accuracy: 0.9681 - val_loss: 1.0200 - val_categorical_accuracy: 0.9736\n",
      "Epoch 4/200\n",
      "24880/25141 [============================>.] - ETA: 0s - loss: 1.0080 - categorical_accuracy: 0.9690Epoch 00003: val_loss improved from 1.01996 to 1.01781, saving model to sub_x1.h5\n",
      "roc-auc: 0.7215 \n",
      "\n",
      "25141/25141 [==============================] - 4s - loss: 1.0082 - categorical_accuracy: 0.9690 - val_loss: 1.0178 - val_categorical_accuracy: 0.9755\n",
      "Epoch 5/200\n",
      "24720/25141 [============================>.] - ETA: 0s - loss: 1.0039 - categorical_accuracy: 0.9702Epoch 00004: val_loss improved from 1.01781 to 1.01624, saving model to sub_x1.h5\n",
      "roc-auc: 0.7811 \n",
      "\n",
      "25141/25141 [==============================] - 5s - loss: 1.0038 - categorical_accuracy: 0.9700 - val_loss: 1.0162 - val_categorical_accuracy: 0.9783\n",
      "Epoch 6/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9982 - categorical_accuracy: 0.9706Epoch 00005: val_loss improved from 1.01624 to 1.01386, saving model to sub_x1.h5\n",
      "roc-auc: 0.776 \n",
      "\n",
      "25141/25141 [==============================] - 9s - loss: 0.9988 - categorical_accuracy: 0.9706 - val_loss: 1.0139 - val_categorical_accuracy: 0.9785\n",
      "Epoch 7/200\n",
      "25120/25141 [============================>.] - ETA: 0s - loss: 0.9981 - categorical_accuracy: 0.9715Epoch 00006: val_loss did not improve\n",
      "roc-auc: 0.7787 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9978 - categorical_accuracy: 0.9714 - val_loss: 1.0141 - val_categorical_accuracy: 0.9785\n",
      "Epoch 8/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9955 - categorical_accuracy: 0.9722Epoch 00007: val_loss improved from 1.01386 to 1.01273, saving model to sub_x1.h5\n",
      "roc-auc: 0.742 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9957 - categorical_accuracy: 0.9722 - val_loss: 1.0127 - val_categorical_accuracy: 0.9790\n",
      "Epoch 9/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9954 - categorical_accuracy: 0.9732Epoch 00008: val_loss improved from 1.01273 to 1.01121, saving model to sub_x1.h5\n",
      "roc-auc: 0.7698 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9937 - categorical_accuracy: 0.9732 - val_loss: 1.0112 - val_categorical_accuracy: 0.9787\n",
      "Epoch 10/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9926 - categorical_accuracy: 0.9739Epoch 00009: val_loss did not improve\n",
      "roc-auc: 0.7849 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9922 - categorical_accuracy: 0.9739 - val_loss: 1.0123 - val_categorical_accuracy: 0.9790\n",
      "Epoch 11/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9929 - categorical_accuracy: 0.9749Epoch 00010: val_loss improved from 1.01121 to 1.01046, saving model to sub_x1.h5\n",
      "roc-auc: 0.8311 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9924 - categorical_accuracy: 0.9749 - val_loss: 1.0105 - val_categorical_accuracy: 0.9789\n",
      "Epoch 12/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9874 - categorical_accuracy: 0.9750Epoch 00011: val_loss improved from 1.01046 to 1.01010, saving model to sub_x1.h5\n",
      "roc-auc: 0.7231 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9882 - categorical_accuracy: 0.9749 - val_loss: 1.0101 - val_categorical_accuracy: 0.9791\n",
      "Epoch 13/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9874 - categorical_accuracy: 0.9754Epoch 00012: val_loss did not improve\n",
      "roc-auc: 0.773 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9877 - categorical_accuracy: 0.9752 - val_loss: 1.0109 - val_categorical_accuracy: 0.9787\n",
      "Epoch 14/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9889 - categorical_accuracy: 0.9762Epoch 00013: val_loss did not improve\n",
      "roc-auc: 0.8119 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9872 - categorical_accuracy: 0.9762 - val_loss: 1.0124 - val_categorical_accuracy: 0.9791\n",
      "Epoch 15/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9895 - categorical_accuracy: 0.9760Epoch 00014: val_loss did not improve\n",
      "roc-auc: 0.7059 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9895 - categorical_accuracy: 0.9761 - val_loss: 1.0109 - val_categorical_accuracy: 0.9791\n",
      "Epoch 16/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9857 - categorical_accuracy: 0.9752Epoch 00015: val_loss did not improve\n",
      "roc-auc: 0.7706 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9854 - categorical_accuracy: 0.9752 - val_loss: 1.0116 - val_categorical_accuracy: 0.9791\n",
      "Epoch 17/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9860 - categorical_accuracy: 0.9760Epoch 00016: val_loss did not improve\n",
      "roc-auc: 0.699 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9857 - categorical_accuracy: 0.9761 - val_loss: 1.0102 - val_categorical_accuracy: 0.9791\n",
      "Epoch 18/200\n",
      "25120/25141 [============================>.] - ETA: 0s - loss: 0.9838 - categorical_accuracy: 0.9764Epoch 00017: val_loss did not improve\n",
      "roc-auc: 0.7046 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9839 - categorical_accuracy: 0.9764 - val_loss: 1.0110 - val_categorical_accuracy: 0.9791\n",
      "Epoch 19/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9833 - categorical_accuracy: 0.9766Epoch 00018: val_loss did not improve\n",
      "roc-auc: 0.7665 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9847 - categorical_accuracy: 0.9766 - val_loss: 1.0117 - val_categorical_accuracy: 0.9791\n",
      "Epoch 20/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9846 - categorical_accuracy: 0.9775Epoch 00019: val_loss did not improve\n",
      "roc-auc: 0.8004 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9832 - categorical_accuracy: 0.9775 - val_loss: 1.0102 - val_categorical_accuracy: 0.9791\n",
      "Epoch 21/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9837 - categorical_accuracy: 0.9775Epoch 00020: val_loss improved from 1.01010 to 1.00950, saving model to sub_x1.h5\n",
      "roc-auc: 0.7652 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9835 - categorical_accuracy: 0.9776 - val_loss: 1.0095 - val_categorical_accuracy: 0.9791\n",
      "Epoch 22/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9813 - categorical_accuracy: 0.9770Epoch 00021: val_loss did not improve\n",
      "roc-auc: 0.7629 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9806 - categorical_accuracy: 0.9770 - val_loss: 1.0105 - val_categorical_accuracy: 0.9791\n",
      "Epoch 23/200\n",
      "25120/25141 [============================>.] - ETA: 0s - loss: 0.9800 - categorical_accuracy: 0.9771Epoch 00022: val_loss did not improve\n",
      "roc-auc: 0.7624 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9799 - categorical_accuracy: 0.9770 - val_loss: 1.0108 - val_categorical_accuracy: 0.9791\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3bcc36c090>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=200, batch_size=80, verbose=1, shuffle=True, validation_split=0.3, callbacks=[mck,estop,roc_callback(training_data=(x_train,y_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.763827757657\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('sub_x1.h5')\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = roc_auc_score(y_test,y_pred,average='weighted')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.concatenate((tfidf_train_1[train_clean_index[28670:28670*2],:], tfidf_train_1[train_toxic_index,:]), axis=0)\n",
    "y = np.concatenate((np.array(train.iloc[train_clean_index[28670:28670*2],2:-1]), np.array(train.iloc[train_toxic_index,2:-1])), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               25728     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 59,526\n",
      "Trainable params: 59,526\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(200,))\n",
    "\n",
    "a = Dense(128,activation='tanh', input_shape=(None,200))(inputs)\n",
    "a = Dropout(0.6)(a)\n",
    "a = Dense(128,activation='tanh')(a)\n",
    "a = Dropout(0.6)(a)\n",
    "a = Dense(128,activation='tanh')(a)\n",
    "a = Dropout(0.6)(a)\n",
    "output= Dense(6, activation='sigmoid')(a)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "mck = ModelCheckpoint('sub_x2.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "estop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25141 samples, validate on 10775 samples\n",
      "Epoch 1/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 1.0823 - categorical_accuracy: 0.9300Epoch 00000: val_loss improved from inf to 1.02349, saving model to sub_x2.h5\n",
      "roc-auc: 0.6378 \n",
      "\n",
      "25141/25141 [==============================] - 13s - loss: 1.0823 - categorical_accuracy: 0.9301 - val_loss: 1.0235 - val_categorical_accuracy: 0.9725\n",
      "Epoch 2/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 1.0267 - categorical_accuracy: 0.9623Epoch 00001: val_loss improved from 1.02349 to 1.00086, saving model to sub_x2.h5\n",
      "roc-auc: 0.6812 \n",
      "\n",
      "25141/25141 [==============================] - 12s - loss: 1.0262 - categorical_accuracy: 0.9624 - val_loss: 1.0009 - val_categorical_accuracy: 0.9658\n",
      "Epoch 3/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 1.0120 - categorical_accuracy: 0.9619Epoch 00002: val_loss improved from 1.00086 to 0.99424, saving model to sub_x2.h5\n",
      "roc-auc: 0.7117 \n",
      "\n",
      "25141/25141 [==============================] - 12s - loss: 1.0122 - categorical_accuracy: 0.9619 - val_loss: 0.9942 - val_categorical_accuracy: 0.9715\n",
      "Epoch 4/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 1.0049 - categorical_accuracy: 0.9603Epoch 00003: val_loss did not improve\n",
      "roc-auc: 0.7204 \n",
      "\n",
      "25141/25141 [==============================] - 12s - loss: 1.0052 - categorical_accuracy: 0.9602 - val_loss: 0.9974 - val_categorical_accuracy: 0.9705\n",
      "Epoch 5/200\n",
      "25120/25141 [============================>.] - ETA: 0s - loss: 1.0000 - categorical_accuracy: 0.9633Epoch 00004: val_loss did not improve\n",
      "roc-auc: 0.7413 \n",
      "\n",
      "25141/25141 [==============================] - 13s - loss: 0.9999 - categorical_accuracy: 0.9632 - val_loss: 0.9950 - val_categorical_accuracy: 0.9777\n",
      "Epoch 6/200\n",
      "25120/25141 [============================>.] - ETA: 0s - loss: 0.9966 - categorical_accuracy: 0.9672Epoch 00005: val_loss improved from 0.99424 to 0.99232, saving model to sub_x2.h5\n",
      "roc-auc: 0.7773 \n",
      "\n",
      "25141/25141 [==============================] - 13s - loss: 0.9968 - categorical_accuracy: 0.9672 - val_loss: 0.9923 - val_categorical_accuracy: 0.9783\n",
      "Epoch 7/200\n",
      "25120/25141 [============================>.] - ETA: 0s - loss: 0.9956 - categorical_accuracy: 0.9682Epoch 00006: val_loss improved from 0.99232 to 0.98734, saving model to sub_x2.h5\n",
      "roc-auc: 0.7527 \n",
      "\n",
      "25141/25141 [==============================] - 13s - loss: 0.9957 - categorical_accuracy: 0.9682 - val_loss: 0.9873 - val_categorical_accuracy: 0.9747\n",
      "Epoch 8/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9931 - categorical_accuracy: 0.9674Epoch 00007: val_loss did not improve\n",
      "roc-auc: 0.7785 \n",
      "\n",
      "25141/25141 [==============================] - 13s - loss: 0.9922 - categorical_accuracy: 0.9674 - val_loss: 0.9885 - val_categorical_accuracy: 0.9790\n",
      "Epoch 9/200\n",
      "25120/25141 [============================>.] - ETA: 0s - loss: 0.9932 - categorical_accuracy: 0.9702Epoch 00008: val_loss did not improve\n",
      "roc-auc: 0.7974 \n",
      "\n",
      "25141/25141 [==============================] - 13s - loss: 0.9933 - categorical_accuracy: 0.9702 - val_loss: 0.9887 - val_categorical_accuracy: 0.9793\n",
      "Epoch 10/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9917 - categorical_accuracy: 0.9708Epoch 00009: val_loss did not improve\n",
      "roc-auc: 0.7877 \n",
      "\n",
      "25141/25141 [==============================] - 13s - loss: 0.9917 - categorical_accuracy: 0.9708 - val_loss: 0.9893 - val_categorical_accuracy: 0.9787\n",
      "Epoch 11/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9887 - categorical_accuracy: 0.9693Epoch 00010: val_loss did not improve\n",
      "roc-auc: 0.7754 \n",
      "\n",
      "25141/25141 [==============================] - 13s - loss: 0.9892 - categorical_accuracy: 0.9693 - val_loss: 0.9891 - val_categorical_accuracy: 0.9795\n",
      "Epoch 12/200\n",
      "25120/25141 [============================>.] - ETA: 0s - loss: 0.9882 - categorical_accuracy: 0.9720Epoch 00011: val_loss improved from 0.98734 to 0.98686, saving model to sub_x2.h5\n",
      "roc-auc: 0.7864 \n",
      "\n",
      "25141/25141 [==============================] - 13s - loss: 0.9884 - categorical_accuracy: 0.9720 - val_loss: 0.9869 - val_categorical_accuracy: 0.9797\n",
      "Epoch 13/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9883 - categorical_accuracy: 0.9726Epoch 00012: val_loss did not improve\n",
      "roc-auc: 0.7641 \n",
      "\n",
      "25141/25141 [==============================] - 13s - loss: 0.9879 - categorical_accuracy: 0.9726 - val_loss: 0.9886 - val_categorical_accuracy: 0.9790\n",
      "Epoch 14/200\n",
      "25120/25141 [============================>.] - ETA: 0s - loss: 0.9852 - categorical_accuracy: 0.9724Epoch 00013: val_loss did not improve\n",
      "roc-auc: 0.8031 \n",
      "\n",
      "25141/25141 [==============================] - 13s - loss: 0.9855 - categorical_accuracy: 0.9724 - val_loss: 0.9884 - val_categorical_accuracy: 0.9803\n",
      "Epoch 15/200\n",
      "25120/25141 [============================>.] - ETA: 0s - loss: 0.9860 - categorical_accuracy: 0.9744Epoch 00014: val_loss did not improve\n",
      "roc-auc: 0.8094 \n",
      "\n",
      "25141/25141 [==============================] - 13s - loss: 0.9861 - categorical_accuracy: 0.9745 - val_loss: 0.9895 - val_categorical_accuracy: 0.9802\n",
      "Epoch 16/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9849 - categorical_accuracy: 0.9738 ETA: 2s - loss:Epoch 00015: val_loss did not improve\n",
      "roc-auc: 0.7578 \n",
      "\n",
      "25141/25141 [==============================] - 14s - loss: 0.9853 - categorical_accuracy: 0.9738 - val_loss: 0.9883 - val_categorical_accuracy: 0.9800\n",
      "Epoch 17/200\n",
      "25040/25141 [============================>.] - ETA: 0s - loss: 0.9835 - categorical_accuracy: 0.9735Epoch 00016: val_loss did not improve\n",
      "roc-auc: 0.8047 \n",
      "\n",
      "25141/25141 [==============================] - 13s - loss: 0.9828 - categorical_accuracy: 0.9734 - val_loss: 0.9886 - val_categorical_accuracy: 0.9801\n",
      "Epoch 18/200\n",
      "25120/25141 [============================>.] - ETA: 0s - loss: 0.9829 - categorical_accuracy: 0.9745Epoch 00017: val_loss did not improve\n",
      "roc-auc: 0.8167 \n",
      "\n",
      "25141/25141 [==============================] - 13s - loss: 0.9824 - categorical_accuracy: 0.9745 - val_loss: 0.9893 - val_categorical_accuracy: 0.9793\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b86b7c150>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=200, batch_size=80, verbose=1, shuffle=True, validation_split=0.3, callbacks=[mck,estop,roc_callback(training_data=(x_train,y_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772809472792\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('sub_x2.h5')\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = roc_auc_score(y_test,y_pred,average='weighted')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.concatenate((tfidf_train_1[train_clean_index[28670*2:28670*3],:], tfidf_train_1[train_toxic_index,:]), axis=0)\n",
    "y = np.concatenate((np.array(train.iloc[train_clean_index[28670*2:28670*3],2:-1]), np.array(train.iloc[train_toxic_index,2:-1])), axis=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2, random_state=634)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               25728     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 76,038\n",
      "Trainable params: 76,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(200,))\n",
    "\n",
    "a = Dense(128,activation='tanh', input_shape=(None,200))(inputs)\n",
    "a = Dropout(0.5)(a)\n",
    "a = Dense(128,activation='tanh')(a)\n",
    "a = Dropout(0.5)(a)\n",
    "a = Dense(128,activation='tanh')(a)\n",
    "a = Dropout(0.5)(a)\n",
    "a = Dense(128,activation='tanh')(a)\n",
    "a = Dropout(0.5)(a)\n",
    "output= Dense(6, activation='sigmoid')(a)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "mck = ModelCheckpoint('sub_x3.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "estop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25141 samples, validate on 10775 samples\n",
      "Epoch 1/200\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0673 - categorical_accuracy: 0.9566Epoch 00000: val_loss improved from inf to 1.00154, saving model to sub_x3.h5\n",
      "roc-auc: 0.6937 \n",
      "\n",
      "25141/25141 [==============================] - 6s - loss: 1.0669 - categorical_accuracy: 0.9568 - val_loss: 1.0015 - val_categorical_accuracy: 0.9745\n",
      "Epoch 2/200\n",
      "24850/25141 [============================>.] - ETA: 0s - loss: 1.0191 - categorical_accuracy: 0.9643Epoch 00001: val_loss improved from 1.00154 to 0.98866, saving model to sub_x3.h5\n",
      "roc-auc: 0.7108 \n",
      "\n",
      "25141/25141 [==============================] - 7s - loss: 1.0210 - categorical_accuracy: 0.9644 - val_loss: 0.9887 - val_categorical_accuracy: 0.9793\n",
      "Epoch 3/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 1.0098 - categorical_accuracy: 0.9649Epoch 00002: val_loss improved from 0.98866 to 0.98690, saving model to sub_x3.h5\n",
      "roc-auc: 0.7133 \n",
      "\n",
      "25141/25141 [==============================] - 21s - loss: 1.0108 - categorical_accuracy: 0.9649 - val_loss: 0.9869 - val_categorical_accuracy: 0.9783\n",
      "Epoch 4/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 1.0041 - categorical_accuracy: 0.9656 ETA: 3s - loss: 1.0 - ETA: 1s - loss: 1.0046 - categorica - ETA: 1s - loss:Epoch 00003: val_loss improved from 0.98690 to 0.98254, saving model to sub_x3.h5\n",
      "roc-auc: 0.8045 \n",
      "\n",
      "25141/25141 [==============================] - 27s - loss: 1.0038 - categorical_accuracy: 0.9656 - val_loss: 0.9825 - val_categorical_accuracy: 0.9807\n",
      "Epoch 5/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 1.0021 - categorical_accuracy: 0.9720 - ETA: 17s - loss: 1.0092 - categorical - ETA:  - ETAEpoch 00004: val_loss did not improve\n",
      "roc-auc: 0.8054 \n",
      "\n",
      "25141/25141 [==============================] - 27s - loss: 1.0015 - categorical_accuracy: 0.9719 - val_loss: 0.9887 - val_categorical_accuracy: 0.9808\n",
      "Epoch 6/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 0.9990 - categorical_accuracy: 0.9725 ETA: 5s - loss: 0.9886 - cateEpoch 00005: val_loss did not improve\n",
      "roc-auc: 0.8098 \n",
      "\n",
      "25141/25141 [==============================] - 31s - loss: 0.9991 - categorical_accuracy: 0.9724 - val_loss: 0.9833 - val_categorical_accuracy: 0.9805\n",
      "Epoch 7/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 0.9975 - categorical_accuracy: 0.9756 ETA: 2s - loss: 0.9898 - categorical_accuracyEpoch 00006: val_loss did not improve\n",
      "roc-auc: 0.7438 \n",
      "\n",
      "25141/25141 [==============================] - 28s - loss: 0.9959 - categorical_accuracy: 0.9756 - val_loss: 0.9830 - val_categorical_accuracy: 0.9807\n",
      "Epoch 8/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 0.9966 - categorical_accuracy: 0.9743 ETA: 0s - loss: 0.9967 - categorical_Epoch 00007: val_loss did not improve\n",
      "roc-auc: 0.8101 \n",
      "\n",
      "25141/25141 [==============================] - 28s - loss: 0.9963 - categorical_accuracy: 0.9743 - val_loss: 0.9829 - val_categorical_accuracy: 0.9812\n",
      "Epoch 9/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 0.9949 - categorical_accuracy: 0.9767Epoch 00008: val_loss improved from 0.98254 to 0.97906, saving model to sub_x3.h5\n",
      "roc-auc: 0.8145 \n",
      "\n",
      "25141/25141 [==============================] - 29s - loss: 0.9945 - categorical_accuracy: 0.9767 - val_loss: 0.9791 - val_categorical_accuracy: 0.9812\n",
      "Epoch 10/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 0.9923 - categorical_accuracy: 0.9771Epoch 00009: val_loss did not improve\n",
      "roc-auc: 0.7836 \n",
      "\n",
      "25141/25141 [==============================] - 28s - loss: 0.9922 - categorical_accuracy: 0.9772 - val_loss: 0.9795 - val_categorical_accuracy: 0.9812\n",
      "Epoch 11/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 0.9913 - categorical_accuracy: 0.9764 - ETA: 18s - loss - ETA: 4s - loss: 0.986 - ETA: 3s - - ETA: 1s - lEpoch 00010: val_loss did not improve\n",
      "roc-auc: 0.7714 \n",
      "\n",
      "25141/25141 [==============================] - 28s - loss: 0.9916 - categorical_accuracy: 0.9765 - val_loss: 0.9826 - val_categorical_accuracy: 0.9812\n",
      "Epoch 12/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 0.9919 - categorical_accuracy: 0.9766Epoch 00011: val_loss did not improve\n",
      "roc-auc: 0.7998 \n",
      "\n",
      "25141/25141 [==============================] - 28s - loss: 0.9920 - categorical_accuracy: 0.9765 - val_loss: 0.9803 - val_categorical_accuracy: 0.9812\n",
      "Epoch 13/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 0.9897 - categorical_accuracy: 0.9768Epoch 00012: val_loss did not improve\n",
      "roc-auc: 0.7855 \n",
      "\n",
      "25141/25141 [==============================] - 28s - loss: 0.9894 - categorical_accuracy: 0.9769 - val_loss: 0.9819 - val_categorical_accuracy: 0.9812\n",
      "Epoch 14/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 0.9909 - categorical_accuracy: 0.9773Epoch 00013: val_loss did not improve\n",
      "roc-auc: 0.7527 \n",
      "\n",
      "25141/25141 [==============================] - 28s - loss: 0.9909 - categorical_accuracy: 0.9773 - val_loss: 0.9829 - val_categorical_accuracy: 0.9812\n",
      "Epoch 15/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 0.9895 - categorical_accuracy: 0.9776 ETA - ETA: 2s - loss: 0.9877 -  - ETA: 1s - loss: 0.9897 - caEpoch 00014: val_loss did not improve\n",
      "roc-auc: 0.7539 \n",
      "\n",
      "25141/25141 [==============================] - 29s - loss: 0.9895 - categorical_accuracy: 0.9775 - val_loss: 0.9814 - val_categorical_accuracy: 0.9812\n",
      "Epoch 16/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 0.9883 - categorical_accuracy: 0.9772 ETA: 1s - loss: 0.9Epoch 00015: val_loss did not improve\n",
      "roc-auc: 0.7333 \n",
      "\n",
      "25141/25141 [==============================] - 28s - loss: 0.9894 - categorical_accuracy: 0.9772 - val_loss: 0.9800 - val_categorical_accuracy: 0.9812\n",
      "Epoch 17/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 0.9878 - categorical_accuracy: 0.9780Epoch 00016: val_loss did not improve\n",
      "roc-auc: 0.7614 \n",
      "\n",
      "25141/25141 [==============================] - 28s - loss: 0.9883 - categorical_accuracy: 0.9780 - val_loss: 0.9830 - val_categorical_accuracy: 0.9812\n",
      "Epoch 18/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 0.9876 - categorical_accuracy: 0.9782 - E - ETA: 1s - lEpoch 00017: val_loss did not improve\n",
      "roc-auc: 0.7825 \n",
      "\n",
      "25141/25141 [==============================] - 28s - loss: 0.9875 - categorical_accuracy: 0.9782 - val_loss: 0.9802 - val_categorical_accuracy: 0.9812\n",
      "Epoch 19/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 0.9877 - categorical_accuracy: 0.9782Epoch 00018: val_loss did not improve\n",
      "roc-auc: 0.7914 \n",
      "\n",
      "25141/25141 [==============================] - 28s - loss: 0.9874 - categorical_accuracy: 0.9782 - val_loss: 0.9835 - val_categorical_accuracy: 0.9812\n",
      "Epoch 20/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 0.9865 - categorical_accuracy: 0.9783Epoch 00019: val_loss did not improve\n",
      "roc-auc: 0.7322 \n",
      "\n",
      "25141/25141 [==============================] - 30s - loss: 0.9860 - categorical_accuracy: 0.9783 - val_loss: 0.9830 - val_categorical_accuracy: 0.9811\n",
      "Epoch 00019: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b2461cf50>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=200, batch_size=50, verbose=1, shuffle=True, validation_split=0.3, callbacks=[mck,estop,roc_callback(training_data=(x_train,y_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806991016792\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('sub_x3.h5')\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = roc_auc_score(y_test,y_pred,average='weighted')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.concatenate((tfidf_train_1[train_clean_index[28670*3:28670*4],:], tfidf_train_1[train_toxic_index,:]), axis=0)\n",
    "y = np.concatenate((np.array(train.iloc[train_clean_index[28670*3:28670*4],2:-1]), np.array(train.iloc[train_toxic_index,2:-1])), axis=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2, random_state=1264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               25728     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 76,038\n",
      "Trainable params: 76,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(200,))\n",
    "\n",
    "a = Dense(128,activation='tanh', input_shape=(None,200))(inputs)\n",
    "a = Dropout(0.6)(a)\n",
    "a = Dense(128,activation='tanh')(a)\n",
    "a = Dropout(0.6)(a)\n",
    "a = Dense(128,activation='tanh')(a)\n",
    "a = Dropout(0.6)(a)\n",
    "a = Dense(128,activation='tanh')(a)\n",
    "a = Dropout(0.6)(a)\n",
    "output= Dense(6, activation='sigmoid')(a)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "mck = ModelCheckpoint('sub_x4.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "estop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25141 samples, validate on 10775 samples\n",
      "Epoch 1/200\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0906 - categorical_accuracy: 0.9518Epoch 00000: val_loss improved from inf to 0.98967, saving model to sub_x4.h5\n",
      "roc-auc: 0.6472 \n",
      "\n",
      "25141/25141 [==============================] - 6s - loss: 1.0896 - categorical_accuracy: 0.9520 - val_loss: 0.9897 - val_categorical_accuracy: 0.9761\n",
      "Epoch 2/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 1.0451 - categorical_accuracy: 0.9683Epoch 00001: val_loss improved from 0.98967 to 0.96371, saving model to sub_x4.h5\n",
      "roc-auc: 0.6804 \n",
      "\n",
      "25141/25141 [==============================] - 6s - loss: 1.0448 - categorical_accuracy: 0.9683 - val_loss: 0.9637 - val_categorical_accuracy: 0.9736\n",
      "Epoch 3/200\n",
      "25000/25141 [============================>.] - ETA: 0s - loss: 1.0301 - categorical_accuracy: 0.9700Epoch 00002: val_loss did not improve\n",
      "roc-auc: 0.741 \n",
      "\n",
      "25141/25141 [==============================] - 6s - loss: 1.0295 - categorical_accuracy: 0.9700 - val_loss: 0.9641 - val_categorical_accuracy: 0.9771\n",
      "Epoch 4/200\n",
      "24800/25141 [============================>.] - ETA: 0s - loss: 1.0226 - categorical_accuracy: 0.9712Epoch 00003: val_loss improved from 0.96371 to 0.95927, saving model to sub_x4.h5\n",
      "roc-auc: 0.6969 \n",
      "\n",
      "25141/25141 [==============================] - 6s - loss: 1.0237 - categorical_accuracy: 0.9711 - val_loss: 0.9593 - val_categorical_accuracy: 0.9770\n",
      "Epoch 5/200\n",
      "24950/25141 [============================>.] - ETA: 0s - loss: 1.0220 - categorical_accuracy: 0.9717Epoch 00004: val_loss did not improve\n",
      "roc-auc: 0.7619 \n",
      "\n",
      "25141/25141 [==============================] - 6s - loss: 1.0208 - categorical_accuracy: 0.9718 - val_loss: 0.9655 - val_categorical_accuracy: 0.9770\n",
      "Epoch 6/200\n",
      "24900/25141 [============================>.] - ETA: 0s - loss: 1.0143 - categorical_accuracy: 0.9748Epoch 00005: val_loss improved from 0.95927 to 0.95922, saving model to sub_x4.h5\n",
      "roc-auc: 0.7564 \n",
      "\n",
      "25141/25141 [==============================] - 6s - loss: 1.0157 - categorical_accuracy: 0.9748 - val_loss: 0.9592 - val_categorical_accuracy: 0.9772\n",
      "Epoch 7/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 1.0166 - categorical_accuracy: 0.9752Epoch 00006: val_loss improved from 0.95922 to 0.95658, saving model to sub_x4.h5\n",
      "roc-auc: 0.7763 \n",
      "\n",
      "25141/25141 [==============================] - 7s - loss: 1.0159 - categorical_accuracy: 0.9752 - val_loss: 0.9566 - val_categorical_accuracy: 0.9774\n",
      "Epoch 8/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 1.0109 - categorical_accuracy: 0.9760Epoch 00007: val_loss improved from 0.95658 to 0.95599, saving model to sub_x4.h5\n",
      "roc-auc: 0.7468 \n",
      "\n",
      "25141/25141 [==============================] - 13s - loss: 1.0109 - categorical_accuracy: 0.9759 - val_loss: 0.9560 - val_categorical_accuracy: 0.9773\n",
      "Epoch 9/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 1.0112 - categorical_accuracy: 0.9760Epoch 00008: val_loss improved from 0.95599 to 0.95358, saving model to sub_x4.h5\n",
      "roc-auc: 0.7777 \n",
      "\n",
      "25141/25141 [==============================] - 19s - loss: 1.0118 - categorical_accuracy: 0.9759 - val_loss: 0.9536 - val_categorical_accuracy: 0.9775\n",
      "Epoch 10/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 1.0083 - categorical_accuracy: 0.9772Epoch 00009: val_loss did not improve\n",
      "roc-auc: 0.7665 \n",
      "\n",
      "25141/25141 [==============================] - 20s - loss: 1.0088 - categorical_accuracy: 0.9772 - val_loss: 0.9554 - val_categorical_accuracy: 0.9775\n",
      "Epoch 11/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 1.0077 - categorical_accuracy: 0.9773Epoch 00010: val_loss improved from 0.95358 to 0.95275, saving model to sub_x4.h5\n",
      "roc-auc: 0.7743 \n",
      "\n",
      "25141/25141 [==============================] - 20s - loss: 1.0079 - categorical_accuracy: 0.9772 - val_loss: 0.9527 - val_categorical_accuracy: 0.9775\n",
      "Epoch 12/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 1.0048 - categorical_accuracy: 0.9776Epoch 00011: val_loss improved from 0.95275 to 0.95251, saving model to sub_x4.h5\n",
      "roc-auc: 0.8114 \n",
      "\n",
      "25141/25141 [==============================] - 20s - loss: 1.0052 - categorical_accuracy: 0.9776 - val_loss: 0.9525 - val_categorical_accuracy: 0.9772\n",
      "Epoch 13/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 1.0058 - categorical_accuracy: 0.9779Epoch 00012: val_loss did not improve\n",
      "roc-auc: 0.8167 \n",
      "\n",
      "25141/25141 [==============================] - 20s - loss: 1.0061 - categorical_accuracy: 0.9779 - val_loss: 0.9526 - val_categorical_accuracy: 0.9774\n",
      "Epoch 14/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 1.0049 - categorical_accuracy: 0.9780Epoch 00013: val_loss did not improve\n",
      "roc-auc: 0.7948 \n",
      "\n",
      "25141/25141 [==============================] - 20s - loss: 1.0040 - categorical_accuracy: 0.9781 - val_loss: 0.9565 - val_categorical_accuracy: 0.9775\n",
      "Epoch 15/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 1.0033 - categorical_accuracy: 0.9770Epoch 00014: val_loss did not improve\n",
      "roc-auc: 0.7909 \n",
      "\n",
      "25141/25141 [==============================] - 20s - loss: 1.0032 - categorical_accuracy: 0.9770 - val_loss: 0.9560 - val_categorical_accuracy: 0.9775\n",
      "Epoch 16/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 1.0027 - categorical_accuracy: 0.9778Epoch 00015: val_loss did not improve\n",
      "roc-auc: 0.7848 \n",
      "\n",
      "25141/25141 [==============================] - 20s - loss: 1.0023 - categorical_accuracy: 0.9778 - val_loss: 0.9544 - val_categorical_accuracy: 0.9775\n",
      "Epoch 17/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 1.0009 - categorical_accuracy: 0.9779Epoch 00016: val_loss did not improve\n",
      "roc-auc: 0.7931 \n",
      "\n",
      "25141/25141 [==============================] - 20s - loss: 1.0021 - categorical_accuracy: 0.9779 - val_loss: 0.9556 - val_categorical_accuracy: 0.9775\n",
      "Epoch 18/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 1.0024 - categorical_accuracy: 0.9775 ETA: 0s - loss: 1.0015 - categorical_accuracy: 0.97Epoch 00017: val_loss did not improve\n",
      "roc-auc: 0.8138 \n",
      "\n",
      "25141/25141 [==============================] - 20s - loss: 1.0026 - categorical_accuracy: 0.9774 - val_loss: 0.9549 - val_categorical_accuracy: 0.9775\n",
      "Epoch 19/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 0.9989 - categorical_accuracy: 0.9783Epoch 00018: val_loss did not improve\n",
      "roc-auc: 0.7906 \n",
      "\n",
      "25141/25141 [==============================] - 20s - loss: 1.0001 - categorical_accuracy: 0.9784 - val_loss: 0.9549 - val_categorical_accuracy: 0.9775\n",
      "Epoch 20/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 0.9984 - categorical_accuracy: 0.9776Epoch 00019: val_loss did not improve\n",
      "roc-auc: 0.8208 \n",
      "\n",
      "25141/25141 [==============================] - 20s - loss: 0.9979 - categorical_accuracy: 0.9777 - val_loss: 0.9586 - val_categorical_accuracy: 0.9775\n",
      "Epoch 21/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 0.9993 - categorical_accuracy: 0.9785Epoch 00020: val_loss did not improve\n",
      "roc-auc: 0.7893 \n",
      "\n",
      "25141/25141 [==============================] - 20s - loss: 0.9995 - categorical_accuracy: 0.9786 - val_loss: 0.9560 - val_categorical_accuracy: 0.9775\n",
      "Epoch 22/200\n",
      "25050/25141 [============================>.] - ETA: 0s - loss: 0.9981 - categorical_accuracy: 0.9783Epoch 00021: val_loss did not improve\n",
      "roc-auc: 0.781 \n",
      "\n",
      "25141/25141 [==============================] - 20s - loss: 0.9989 - categorical_accuracy: 0.9783 - val_loss: 0.9568 - val_categorical_accuracy: 0.9775\n",
      "Epoch 23/200\n",
      "25100/25141 [============================>.] - ETA: 0s - loss: 1.0004 - categorical_accuracy: 0.9784Epoch 00022: val_loss did not improve\n",
      "roc-auc: 0.7853 \n",
      "\n",
      "25141/25141 [==============================] - 20s - loss: 1.0005 - categorical_accuracy: 0.9784 - val_loss: 0.9575 - val_categorical_accuracy: 0.9775\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b449a6810>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=200, batch_size=50, verbose=1, shuffle=True, validation_split=0.3, callbacks=[mck,estop,roc_callback(training_data=(x_train,y_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816510195348\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('sub_x4.h5')\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = roc_auc_score(y_test,y_pred,average='weighted')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.concatenate((tfidf_train_1[train_clean_index[28670*4:],:], tfidf_train_1[train_toxic_index,:]), axis=0)\n",
    "y = np.concatenate((np.array(train.iloc[train_clean_index[28670*4:],2:-1]), np.array(train.iloc[train_toxic_index,2:-1])), axis=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2, random_state=2673)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               25728     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 59,526\n",
      "Trainable params: 59,526\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(200,))\n",
    "\n",
    "a = Dense(128,activation='tanh', input_shape=(None,200))(inputs)\n",
    "a = Dropout(0.6)(a)\n",
    "a = Dense(128,activation='tanh')(a)\n",
    "a = Dropout(0.6)(a)\n",
    "a = Dense(128,activation='tanh')(a)\n",
    "a = Dropout(0.6)(a)\n",
    "output= Dense(6, activation='sigmoid')(a)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "mck = ModelCheckpoint('sub_x5.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "estop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25138 samples, validate on 10774 samples\n",
      "Epoch 1/200\n",
      "25000/25138 [============================>.] - ETA: 0s - loss: 1.0810 - categorical_accuracy: 0.9548Epoch 00000: val_loss improved from inf to 1.02134, saving model to sub_x5.h5\n",
      "roc-auc: 0.6555 \n",
      "\n",
      "25138/25138 [==============================] - 6s - loss: 1.0817 - categorical_accuracy: 0.9547 - val_loss: 1.0213 - val_categorical_accuracy: 0.9686\n",
      "Epoch 2/200\n",
      "25000/25138 [============================>.] - ETA: 0s - loss: 1.0324 - categorical_accuracy: 0.9632Epoch 00001: val_loss improved from 1.02134 to 1.00176, saving model to sub_x5.h5\n",
      "roc-auc: 0.6983 \n",
      "\n",
      "25138/25138 [==============================] - 7s - loss: 1.0334 - categorical_accuracy: 0.9632 - val_loss: 1.0018 - val_categorical_accuracy: 0.9695\n",
      "Epoch 3/200\n",
      "25100/25138 [============================>.] - ETA: 0s - loss: 1.0215 - categorical_accuracy: 0.9652Epoch 00002: val_loss improved from 1.00176 to 0.99733, saving model to sub_x5.h5\n",
      "roc-auc: 0.7153 \n",
      "\n",
      "25138/25138 [==============================] - 18s - loss: 1.0214 - categorical_accuracy: 0.9652 - val_loss: 0.9973 - val_categorical_accuracy: 0.9702\n",
      "Epoch 4/200\n",
      "25050/25138 [============================>.] - ETA: 0s - loss: 1.0143 - categorical_accuracy: 0.9629Epoch 00003: val_loss improved from 0.99733 to 0.99339, saving model to sub_x5.h5\n",
      "roc-auc: 0.749 \n",
      "\n",
      "25138/25138 [==============================] - 28s - loss: 1.0144 - categorical_accuracy: 0.9629 - val_loss: 0.9934 - val_categorical_accuracy: 0.9742\n",
      "Epoch 5/200\n",
      "25050/25138 [============================>.] - ETA: 0s - loss: 1.0116 - categorical_accuracy: 0.9686Epoch 00004: val_loss did not improve\n",
      "roc-auc: 0.7513 \n",
      "\n",
      "25138/25138 [==============================] - 18s - loss: 1.0109 - categorical_accuracy: 0.9687 - val_loss: 0.9952 - val_categorical_accuracy: 0.9755\n",
      "Epoch 6/200\n",
      "25100/25138 [============================>.] - ETA: 0s - loss: 1.0073 - categorical_accuracy: 0.9703 ETA: 0s - loss: 1.0095 - categorical_accuracyEpoch 00005: val_loss did not improve\n",
      "roc-auc: 0.7421 \n",
      "\n",
      "25138/25138 [==============================] - 18s - loss: 1.0084 - categorical_accuracy: 0.9703 - val_loss: 0.9946 - val_categorical_accuracy: 0.9763\n",
      "Epoch 7/200\n",
      "25050/25138 [============================>.] - ETA: 0s - loss: 1.0054 - categorical_accuracy: 0.9707Epoch 00006: val_loss improved from 0.99339 to 0.99227, saving model to sub_x5.h5\n",
      "roc-auc: 0.7907 \n",
      "\n",
      "25138/25138 [==============================] - 18s - loss: 1.0079 - categorical_accuracy: 0.9706 - val_loss: 0.9923 - val_categorical_accuracy: 0.9765\n",
      "Epoch 8/200\n",
      "25100/25138 [============================>.] - ETA: 0s - loss: 1.0039 - categorical_accuracy: 0.9723Epoch 00007: val_loss improved from 0.99227 to 0.99015, saving model to sub_x5.h5\n",
      "roc-auc: 0.7839 \n",
      "\n",
      "25138/25138 [==============================] - 18s - loss: 1.0041 - categorical_accuracy: 0.9722 - val_loss: 0.9901 - val_categorical_accuracy: 0.9767\n",
      "Epoch 9/200\n",
      "25050/25138 [============================>.] - ETA: 0s - loss: 1.0056 - categorical_accuracy: 0.9721Epoch 00008: val_loss did not improve\n",
      "roc-auc: 0.7942 \n",
      "\n",
      "25138/25138 [==============================] - 19s - loss: 1.0049 - categorical_accuracy: 0.9722 - val_loss: 0.9902 - val_categorical_accuracy: 0.9770\n",
      "Epoch 10/200\n",
      "25050/25138 [============================>.] - ETA: 0s - loss: 1.0039 - categorical_accuracy: 0.9721 ETA: 0s - loss: 1.0055 - categoriEpoch 00009: val_loss improved from 0.99015 to 0.98961, saving model to sub_x5.h5\n",
      "roc-auc: 0.8223 \n",
      "\n",
      "25138/25138 [==============================] - 18s - loss: 1.0033 - categorical_accuracy: 0.9722 - val_loss: 0.9896 - val_categorical_accuracy: 0.9771\n",
      "Epoch 11/200\n",
      "25100/25138 [============================>.] - ETA: 0s - loss: 1.0019 - categorical_accuracy: 0.9743Epoch 00010: val_loss did not improve\n",
      "roc-auc: 0.7596 \n",
      "\n",
      "25138/25138 [==============================] - 18s - loss: 1.0024 - categorical_accuracy: 0.9742 - val_loss: 0.9922 - val_categorical_accuracy: 0.9772\n",
      "Epoch 12/200\n",
      "25100/25138 [============================>.] - ETA: 0s - loss: 1.0015 - categorical_accuracy: 0.9759Epoch 00011: val_loss did not improve\n",
      "roc-auc: 0.8409 \n",
      "\n",
      "25138/25138 [==============================] - 19s - loss: 1.0007 - categorical_accuracy: 0.9759 - val_loss: 0.9919 - val_categorical_accuracy: 0.9774\n",
      "Epoch 13/200\n",
      "25100/25138 [============================>.] - ETA: 0s - loss: 1.0002 - categorical_accuracy: 0.9762Epoch 00012: val_loss improved from 0.98961 to 0.98916, saving model to sub_x5.h5\n",
      "roc-auc: 0.7903 \n",
      "\n",
      "25138/25138 [==============================] - 19s - loss: 1.0001 - categorical_accuracy: 0.9763 - val_loss: 0.9892 - val_categorical_accuracy: 0.9774\n",
      "Epoch 14/200\n",
      "25100/25138 [============================>.] - ETA: 0s - loss: 1.0021 - categorical_accuracy: 0.9763Epoch 00013: val_loss did not improve\n",
      "roc-auc: 0.7731 \n",
      "\n",
      "25138/25138 [==============================] - 19s - loss: 1.0021 - categorical_accuracy: 0.9763 - val_loss: 0.9900 - val_categorical_accuracy: 0.9774\n",
      "Epoch 15/200\n",
      "25050/25138 [============================>.] - ETA: 0s - loss: 0.9992 - categorical_accuracy: 0.9764Epoch 00014: val_loss did not improve\n",
      "roc-auc: 0.7545 \n",
      "\n",
      "25138/25138 [==============================] - 19s - loss: 0.9992 - categorical_accuracy: 0.9764 - val_loss: 0.9892 - val_categorical_accuracy: 0.9769\n",
      "Epoch 16/200\n",
      "25100/25138 [============================>.] - ETA: 0s - loss: 0.9970 - categorical_accuracy: 0.9765Epoch 00015: val_loss did not improve\n",
      "roc-auc: 0.7542 \n",
      "\n",
      "25138/25138 [==============================] - 19s - loss: 0.9978 - categorical_accuracy: 0.9765 - val_loss: 0.9904 - val_categorical_accuracy: 0.9774\n",
      "Epoch 17/200\n",
      "25050/25138 [============================>.] - ETA: 0s - loss: 0.9982 - categorical_accuracy: 0.9770Epoch 00016: val_loss improved from 0.98916 to 0.98854, saving model to sub_x5.h5\n",
      "roc-auc: 0.7956 \n",
      "\n",
      "25138/25138 [==============================] - 19s - loss: 0.9988 - categorical_accuracy: 0.9770 - val_loss: 0.9885 - val_categorical_accuracy: 0.9774\n",
      "Epoch 18/200\n",
      "25050/25138 [============================>.] - ETA: 0s - loss: 0.9958 - categorical_accuracy: 0.9761Epoch 00017: val_loss did not improve\n",
      "roc-auc: 0.8358 \n",
      "\n",
      "25138/25138 [==============================] - 19s - loss: 0.9958 - categorical_accuracy: 0.9761 - val_loss: 0.9897 - val_categorical_accuracy: 0.9774\n",
      "Epoch 19/200\n",
      "25100/25138 [============================>.] - ETA: 0s - loss: 0.9975 - categorical_accuracy: 0.9773Epoch 00018: val_loss did not improve\n",
      "roc-auc: 0.7966 \n",
      "\n",
      "25138/25138 [==============================] - 19s - loss: 0.9970 - categorical_accuracy: 0.9773 - val_loss: 0.9900 - val_categorical_accuracy: 0.9774\n",
      "Epoch 20/200\n",
      "25050/25138 [============================>.] - ETA: 0s - loss: 0.9952 - categorical_accuracy: 0.9776 ETA: 0s - loss: 0.9Epoch 00019: val_loss did not improve\n",
      "roc-auc: 0.772 \n",
      "\n",
      "25138/25138 [==============================] - 20s - loss: 0.9965 - categorical_accuracy: 0.9775 - val_loss: 0.9891 - val_categorical_accuracy: 0.9774\n",
      "Epoch 21/200\n",
      "25050/25138 [============================>.] - ETA: 0s - loss: 0.9959 - categorical_accuracy: 0.9780Epoch 00020: val_loss did not improve\n",
      "roc-auc: 0.7585 \n",
      "\n",
      "25138/25138 [==============================] - 20s - loss: 0.9957 - categorical_accuracy: 0.9781 - val_loss: 0.9899 - val_categorical_accuracy: 0.9774\n",
      "Epoch 22/200\n",
      "25100/25138 [============================>.] - ETA: 0s - loss: 0.9942 - categorical_accuracy: 0.9777Epoch 00021: val_loss did not improve\n",
      "roc-auc: 0.8235 \n",
      "\n",
      "25138/25138 [==============================] - 19s - loss: 0.9947 - categorical_accuracy: 0.9777 - val_loss: 0.9897 - val_categorical_accuracy: 0.9774\n",
      "Epoch 23/200\n",
      "25100/25138 [============================>.] - ETA: 0s - loss: 0.9929 - categorical_accuracy: 0.9782Epoch 00022: val_loss did not improve\n",
      "roc-auc: 0.7812 \n",
      "\n",
      "25138/25138 [==============================] - 20s - loss: 0.9925 - categorical_accuracy: 0.9781 - val_loss: 0.9910 - val_categorical_accuracy: 0.9774\n",
      "Epoch 24/200\n",
      "25100/25138 [============================>.] - ETA: 0s - loss: 0.9943 - categorical_accuracy: 0.9773Epoch 00023: val_loss improved from 0.98854 to 0.98807, saving model to sub_x5.h5\n",
      "roc-auc: 0.7977 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25138/25138 [==============================] - 20s - loss: 0.9936 - categorical_accuracy: 0.9774 - val_loss: 0.9881 - val_categorical_accuracy: 0.9774\n",
      "Epoch 25/200\n",
      "25000/25138 [============================>.] - ETA: 0s - loss: 0.9956 - categorical_accuracy: 0.9776Epoch 00024: val_loss did not improve\n",
      "roc-auc: 0.7492 \n",
      "\n",
      "25138/25138 [==============================] - 17s - loss: 0.9946 - categorical_accuracy: 0.9776 - val_loss: 0.9899 - val_categorical_accuracy: 0.9774\n",
      "Epoch 26/200\n",
      "25100/25138 [============================>.] - ETA: 0s - loss: 0.9914 - categorical_accuracy: 0.9777Epoch 00025: val_loss did not improve\n",
      "roc-auc: 0.7822 \n",
      "\n",
      "25138/25138 [==============================] - 17s - loss: 0.9910 - categorical_accuracy: 0.9778 - val_loss: 0.9886 - val_categorical_accuracy: 0.9774\n",
      "Epoch 27/200\n",
      "25000/25138 [============================>.] - ETA: 0s - loss: 0.9959 - categorical_accuracy: 0.9774Epoch 00026: val_loss did not improve\n",
      "roc-auc: 0.7801 \n",
      "\n",
      "25138/25138 [==============================] - 18s - loss: 0.9938 - categorical_accuracy: 0.9774 - val_loss: 0.9903 - val_categorical_accuracy: 0.9774\n",
      "Epoch 28/200\n",
      "25100/25138 [============================>.] - ETA: 0s - loss: 0.9950 - categorical_accuracy: 0.9773Epoch 00027: val_loss did not improve\n",
      "roc-auc: 0.7359 \n",
      "\n",
      "25138/25138 [==============================] - 17s - loss: 0.9946 - categorical_accuracy: 0.9773 - val_loss: 0.9898 - val_categorical_accuracy: 0.9774\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b116b9bd0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=200, batch_size=50, verbose=1, shuffle=True, validation_split=0.3, callbacks=[mck,estop,roc_callback(training_data=(x_train,y_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.789489397956\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('sub_x5.h5')\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = roc_auc_score(y_test,y_pred,average='weighted')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('sub_x1.h5')\n",
    "y_pred_1 = model.predict(tfidf_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('sub_x2.h5')\n",
    "y_pred_2 = model.predict(tfidf_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('sub_x3.h5')\n",
    "y_pred_3 = model.predict(tfidf_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('sub_x4.h5')\n",
    "y_pred_4 = model.predict(tfidf_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('sub_x5.h5')\n",
    "y_pred_5 = model.predict(tfidf_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bagging_pred = y_pred_1+y_pred_2+y_pred_3+y_pred_4+y_pred_5\n",
    "bagging_pred = bagging_pred/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(test['id'])\n",
    "\n",
    "for i in range(len(levels)):\n",
    "    sub[levels[i]]=bagging_pred[:,i]\n",
    "sub.to_csv('bagging_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(test['id'])\n",
    "\n",
    "for i in range(len(levels)):\n",
    "    sub[levels[i]]=y_pred_1[:,i]\n",
    "sub.to_csv('pred_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(test['id'])\n",
    "\n",
    "for i in range(len(levels)):\n",
    "    sub[levels[i]]=y_pred_2[:,i]\n",
    "sub.to_csv('pred_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(test['id'])\n",
    "\n",
    "for i in range(len(levels)):\n",
    "    sub[levels[i]]=y_pred_3[:,i]\n",
    "sub.to_csv('pred_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(test['id'])\n",
    "\n",
    "for i in range(len(levels)):\n",
    "    sub[levels[i]]=y_pred_4[:,i]\n",
    "sub.to_csv('pred_4.csv', index=False)\n",
    "\n",
    "#This is the best till now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(test['id'])\n",
    "\n",
    "for i in range(len(levels)):\n",
    "    sub[levels[i]]=y_pred_5[:,i]\n",
    "sub.to_csv('pred_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
